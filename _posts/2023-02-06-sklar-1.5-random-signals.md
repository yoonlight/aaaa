---
marp: true
math: katex
paginate: true
backgroundColor: #fff
title: 1.5 Random Signals
categories:
  - Digital Communications
toc: true
toc_sticky: true
tags:
  - [Sklar, Signals and Spectra, Random Signals]
date: 2023-02-06
last_modified_at: 2023-02-06
---

## Introduction

### Why does we know random signals?

- The main objective of a communication system
  - the transfer of information over a channel
- All useful message signals appear random
- The noise is due to electrical random signals.
- We need to be able to form efficient descriptions of random signals

---

## Random Variables

### A random variable $X(A)$

- random variable과 signal의 관계?
- the functional relationship between a random event $A$ and a real number

---

### The distribution function $F_X(x)$

- random varialbe과 관계
- 언제 쓰는지

$$
F_X(x)=P(X\leq x)
$$

- $P(X\leq x)$: the probability that the value taken by the random variable $X$ is less than or equal to a real number $x$.

#### Properties

1. $0\leq F_X(x)\leq 1$
2. $F_X(x_1)\leq F_X(x_2)\quad\text{if }x_1\leq x_2$
3. $F_X(-\infty)=0$
4. $F_X(+\infty)=1$

---

### The Probability density function (pdf)

- Relation: useful function

$$
P_X(x)={dF_X(x)\over{dx}}
$$

---

- density function: the probability of the event $x_1\leq X\leq x_2$

$$
\begin{split}
  P(x_1\leq X\leq x_2)&=P(X\leq x_2)-P(X\leq x_1)\\
  &=F_X(x_2)-F_X(x_1)\\
  &=\int^{x_2}_{x_1}p_X(x)dx
\end{split}
$$

$$
P(x_1\leq X\leq x_2)\approx p_X(x)\Delta x\\
P(X=x)=p_X(x)dx
$$

---

#### pdf's Properties

1. $p_X(x)\geq 0$
2. $\int^{\infty}_{-\infty}p_X(x)dx=F_X(+\infty)-F_X(-\infty)=1$

---

### Ensemble Averages

- Why?
  - for the purposes of communication system analysis
    - n=1,2

- The mean value or expected value of a random variable $X$

$$
m_X=\mathbf{E}\{X^n\}=\int^{\infty}_{-\infty}x^np_X(x)dx
$$

---

- central moments
  - the moments of the difference between $X$ and $m_X$

#### variance of $X$: the second central moment

- measure of the randomness of the random variable $X$
- by specifiying the variance of a random variable, **we are constraining the width of its pdf**.

$$
\sigma_X^2=\text{var}(X)=\mathbf{E}\{(X-m_X)^2\}=\int^\infty_{-\infty}(x-m_X)^2p_X(x)dx
$$

$$
\sigma_X^2=\mathbf{E}\{X^2\}-m^2_X
$$

---

## Random Processes

- [x] what is?
- [x] Why Use?
- [ ] when use?
- [ ] for what?
- [ ] what relation is?
- [x] vs random variable (or relation)

---

### Definition

> A random process $X(A,t)$ can be viewed as a function of two variables: an event $A$ and time.

- A sample function
  - a single time function
- An ensemble
  - the totality of all sample functions
- A random variable
  - For a specific time $t=t_k$, $X(A, t_k)$
- A number
  - For a specific event $A=A_j$ and a specific time $t=t_k$, $X(A_j, t_k)$

---

### Statistical Averages of a Random Process

- [x] why statistical?

- the value of a random process at any future time is unknown
- a random process whose distribution functions are continuous can be described statistically with a pdf.

---

- A partial description consisting of the mean and autocorrelation function are often adequate for the needs of communication systems

#### mean

$$
\mathbf{E}\{X(t_k)\}=\int^{\infty}_{-\infty}xp_{X_k}(x)dx=m_X(t_k)
$$

#### autocorrelation

$$
R_X(t_1,t_2)=\mathbf{E}\{X(t_1)X(t_2)\}
$$

- a measure of the degree to which two time samples of the same random process are related

---

### Stationarity

- what is?
  - if none of its statistics are affected by a shift in the time origin
- wide-sense stationary (WSS)
  - if two of its statistics (mean, autocorrelation) do not vary with a shift in the time origin.

$$
\mathbf{E}\{X(t)\}=m_x=\text{a constant}\\
R_X(t_1, t_2)=R_X(t_1-t_2)
$$

- why WSS?
  - most of the useful results in communication theory are predicated on random information signals and noise being wide-sense stationary

---

### Autocorrelation of a Wide-Sense Stationary Random Process

- purpose?
  - a similar measure for random processes
  - WSS, autocorrelation function is only a function of the time difference $\tau=t_1-t_2$

$$
R_X(\tau)=\mathbf{E}\{X(t)X(t+\tau)\}\quad\text{for }-\infty < \tau <\infty
$$

- $R_X(\tau)$ indicates the extent to which the random values of the process separated by $\tau$ seconds in time are statistically correlated.

---

#### Properties of the autocorrelation function of a real-valued WSS process

1. $R_x(\tau)=R_x(-\tau)$
2. $R_x(\tau)\leq R_x(0)\text{ for all }\tau$
3. $R_x(\tau)\leftrightarrow G_X(f)$
4. $R_x(0)=\mathbf{E}\{X^2(t)\}$

---

## Time Averaging and Ergodicity

- time averaging
- ergodicity
  - what?
    - its time averages equal its ensemble averages, and the statistical properties of the process can be determined by time averaging over a single smaple function of the process.
  - why?
    - to compute m_X and R_X(\tau) by ensemble averaging not available
  - how?
    - it must be stationary in the strict sense
      - but, for communication systems, WSS
        - mean and autocorrelation functions

---

### Ergodic in the mean and the autocorrelation function

- mean

$$
m_X=\lim_{T\rightarrow\infty}{1/T}\int^\infty_{-\infty}X(t)dt
$$

- auto correlation function

$$
R_X(\tau)=\lim_{T\rightarrow\infty}{1/T}\int^{T/2}_{-T/2}X(t)X(t+\tau)dt
$$

---

### Assumption for the ergodicity of a random process

- The random waveforms are ergodic in the mean and the autocoreelation function in the analysis of most communication signals.

---

## Power Spectral Density and Autocorrelation of a Random Process

### Introduction (Provide)

- PSD enables us to evalutate the signal power that will pass through a network having known frequency characteristics.
- the autocorrelation function gives us frequency information
  - the bandwidth of the signal
- the relative shape of the autocorrelation function tells us something about the bandwidth of the underlying signal
- The autocorrelation function allows us to express a random signal's power spectral density.
  - The area under the PSD curve represents the average power in the signal.

---

### The principal features of PSD functions

1. $G_X(f)\geq0$
2. $G_X(f)=G_X(-f)$
3. $G_X(f)\leftrightarrow R_X(\tau)$
4. $G_X(f)=\int^{\infty}_{-\infty}G_X(f)df$

---

## Noise in Communication Systems

---

### Introducton

- What is noise
- ~~What is noise in communication systems~~
- Why does we know noise?

---

#### What is Noise?

- Unwanted electrical signals that are always present in electrical systems

#### Why does we know the noise?

- The presence of noise superimposed on a signal tends to obscure or mask the signal
- It limits the receiver's ability to make correct symbol decisions.
- limits the rate of information transmission
- eliminate much of the noise or its undesirable effect.

---

#### Thermal noise

- It cannot be eliminated from good engineering design
- it is caused by the thremal motion of electrons in all dissipative components
  - resistors, wires, and so on
- zero-mean Gaussian random process
  - random function whose value $n$ at any arbitrary time $t$ is statistically characterized by the Gaussian pdf

$$
p(n)={1\over{\sigma\sqrt{2\pi}}}\exp{\left[-{1\over{2}} \left({n\over{\sigma}} \right)^2\right]}
$$

- $\sigma^2$: the variance of $n$

---

- A random signal
  - The sum of a Gaussian noise random variable and a dc signal

$$
z=a+n
$$

- $z$: the random signal
- $a$: the dc component
- $n$: the Gaussian noise random variable

#### pdf $p(z)$

$$
p(x)={1\over{\sigma\sqrt{2\pi}}}\exp{\left[-{1\over{2}}\left({z-a\over{\sigma}}\right)^2\right]}
$$

---

#### Central limit theorem

- The Gaussian distribution is often used as the system noise model because of a theorem.
  - under very general conditions the probability distribution of the sum of $j$ statistically independent random variables approaches the Gaussian

---

### White Noise

#### The primary spectral characteristic of thermal noise

- Its power spectral density is the same for all frequencies of interest in most communication systems
- a thermal noise source emanates an equal amount of noise power per unit bandwidth at all frequencies
- its power spectral density $G_n(f)$ is flat for all frequencies

$$
G_n(f)={N_0\over{2}}\quad \text{watts/hertz}
$$

- The factor of 2: $G_n(f)$ is a two-sided power spectral density

---

#### White

- When the noise power has such a uniform spectral density
- white
  - equal amounts of all frequencies within the visible band of electromagnetic radiation

---

#### The autocorrelation function

$$
R_n(\tau)=\mathscr{F}^{-1}\{G_n(f)\}={N_0\over{2}}\delta{(\tau)}
$$

- what does it provide?
  - The autocorrelation function of white noise is given by the inverse Fourier transform of the noise PSD.
  - any two different samples of white noise are always uncorrelated.
  - The average power $P_n$ of white noise is infinite

$$
P_n=\int^{\infty}_{-\infty}{N_0\over{2}}df=\infty
$$

---

#### Additive

> The noise is simply superiposed or added to the signal
